Decentralized Multi-Robot Path Planning (MRPP) is crucial for applications where centralized solutions are infeasible due to scalability or information constraints. This work enhances MRPP by employing Graph Neural Networks (GNNs) for inter-agent coordination. Addressing the rigidity of fixed communication ranges in standard GNNs, this thesis introduces and evaluates a framework incorporating Adaptive Diffusion Convolution (ADC). ADC utilizes a learnable graph diffusion process, parameterized by a diffusion time ($t$), to dynamically adapt information aggregation from neighbors. 

The ADC-enhanced framework is trained via imitation learning on expert demonstrations from Conflict-Based Search (CBS) in 2D grid environments with varying static obstacle densities. Extensive simulations compare ADC variants (ADC-Main with learnable $t$, ADC-FixedT with fixed $t$) against baseline fixed-K GCN models. Key performance metrics include Success Rate (SR), Average Makespan (AM), Flowtime (FT), and computational cost. 

Results show that ADC-based models, especially ADC-FixedT, achieve competitive and often superior performance over fixed-K GCNs. Notably, ADC-FixedT demonstrates improved generalization when transferring from high-density training to lower-density test environments. The adaptive version of the model (ADC-Main) performs well, but it doesn't consistently outperform the fixed version (ADC-FixedT). This shows that the underlying diffusion method is strong by itself. However, making the time parameter $t$ learnable hasn't yet shown clear benefits and needs more investigation. Theoretically, ADC offers guaranteed stability over standard GCNs.

This work, by developing and analyzing a decentralized path planning system with adaptive diffusion, contributes to enhancing the adaptability and real-world potential of multi-robot coordination, aiming for more intelligent robotic swarms in complex, uncertain settings.
