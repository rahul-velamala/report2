% ============================================================
% CHAPTER 3: BACKGROUND AND THEORETICAL FOUNDATIONS
% ============================================================
\chapter{Background and Theoretical Foundations}
\label{chap:background}

This chapter introduces the background needed to understand the proposed decentralized multi-robot path planning framework enhanced with Adaptive Diffusion Convolution (ADC). We begin by introducing fundamental concepts from graph theory, which provide the mathematical language for representing multi-robot systems and their interactions. We then delve into Graph Neural Networks (GNNs), explaining their general architecture and how they process graph-structured data, highlighting the limitations of standard fixed-neighborhood approaches. Subsequently, we explore concepts from graph signal processing and graph diffusion processes, focusing on the graph heat kernel, which forms the basis for ADC. Finally, we detail the Adaptive Diffusion Convolution mechanism itself, explaining how it enables learnable, adaptive information propagation on graphs, followed by a comparison of its theoretical properties against standard GCNs.

\section{Graph Theory Essentials}
\label{sec:graph_theory}

Graphs provide a natural and powerful way to model the relationships and interactions within multi-robot systems.

A \textbf{graph} is formally defined as a pair $G = (V, E)$, where $V$ is the set of vertices (or nodes) and $E \subseteq V \times V$ is the set of edges connecting pairs of vertices. In the context of MRPP, the set of robots typically constitutes the vertex set $V = \{1, ..., N\}$, where $N$ is the total number of robots.

An \textbf{edge} $(i, j) \in E$ signifies a relationship or potential interaction between robot $i$ and robot $j$. In decentralized MRPP, edges often represent the possibility of direct communication or sensing between robots. If the relationship is symmetric (i.e., if robot $i$ can communicate with $j$, then $j$ can communicate with $i$), the graph is \textbf{undirected}. If the relationship has a direction, the graph is \textbf{directed}. In many MRPP scenarios, communication links are bidirectional, leading to undirected graphs.

The structure of the graph, particularly the communication links, can change over time as robots move. Therefore, we often consider a \textbf{time-varying graph} $G_t = (V, E_t)$ at each discrete time step $t$. An edge $(i, j) \in E_t$ might exist if the distance between robots $i$ and $j$ at time $t$, denoted by $\|p_i(t) - p_j(t)\|$, is less than or equal to a predefined communication radius $r_{comm}$.

The structure of a graph $G_t$ at time $t$ is commonly represented using matrices:

\begin{itemize}
    \item \textbf{Adjacency Matrix ($A_t$):} An $N \times N$ matrix where $[A_t]_{ij} = 1$ if $(i, j) \in E_t$ and $i \neq j$, and $[A_t]_{ij} = 0$ otherwise. For undirected graphs, $A_t$ is symmetric ($A_t = A_t^T$). Often, self-loops are added, resulting in $\hat{A}_t = A_t + I$, where $I$ is the identity matrix. This allows a node to consider its own features during aggregation.
    \item \textbf{Degree Matrix ($D_t$):} An $N \times N$ diagonal matrix where the $i$-th diagonal element $[D_t]_{ii}$ represents the degree of node $i$, calculated as $[D_t]_{ii} = \sum_{j=1}^{N} [A_t]_{ij}$. It counts the number of edges connected to node $i$. If self-loops are included via $\hat{A}_t$, the corresponding degree matrix is $\hat{D}_t$, where $[\hat{D}_t]_{ii} = \sum_{j=1}^{N} [\hat{A}_t]_{ij}$.
\end{itemize}

These matrices are fundamental building blocks for defining operations on graph signals and for constructing GNN layers.

\section{Graph Shift Operators (GSOs)}
\label{sec:gsos}

A Graph Shift Operator (GSO) is an $N \times N$ matrix $S$ whose sparsity pattern reflects the underlying graph topology. That is, $[S]_{ij} \neq 0$ only if $i=j$ or $(j, i) \in E$ (or $(i,j) \in E$ depending on convention, but for symmetric operators derived from undirected graphs, it doesn't matter). GSOs represent linear, local operations on graph signals (features defined on the nodes). Applying a GSO $S$ to a matrix of node features $X \in \mathbb{R}^{N \times F}$ (where each row is a node's feature vector) results in $SX$, where the features at each node become a linear combination of features from its neighbors (as defined by the non-zero entries in $S$).

Common GSOs derived from the graph structure at time $t$ include:
\begin{itemize}
    \item \textbf{Adjacency Matrix ($A_t$ or $\hat{A}_t$):} Represents aggregation from direct neighbors (and self if $\hat{A}_t$ is used).
    \item \textbf{Graph Laplacian Matrices:} Laplacians capture notions of smoothness and variation of signals over the graph.
        \begin{itemize}
        
            \item \textbf{Symmetrically Normalized Adjacency Matrix ($\tilde{A}_t$):} Used prominently in GCNs, defined as $\tilde{A}_t = \hat{D}_t^{-1/2} \hat{A}_t \hat{D}_t^{-1/2}$ (equivalent to Eq. 1 in the concise paper, assuming $\hat{A}_t$ and $\hat{D}_t$). This normalization helps stabilize learning and accounts for varying node degrees.
            \item \textbf{Symmetrically Normalized Laplacian ($L_{norm, t}$):} Defined as $L_{norm, t} = I - \tilde{A}_t = I - \hat{D}_t^{-1/2} \hat{A}_t \hat{D}_t^{-1/2}$ (equivalent to Eq. 2). Its eigenvalues are bounded between 0 and 2, which is beneficial for analysis and stability.
        \end{itemize}
\end{itemize}
The choice of GSO fundamentally influences how information propagates within a GNN.

\section{Graph Neural Networks (GNNs)}
\label{sec:gnns}

Graph Neural Networks (GNNs) are a class of deep learning models designed to operate directly on graph-structured data. They leverage the graph topology to learn representations for nodes, edges, or entire graphs. The core idea behind most GNNs is the \textbf{message passing} paradigm \cite{Gilmer2017MessagePassing}, where nodes iteratively aggregate information from their local neighborhoods and update their own representations.

\subsection{General Framework}
A typical GNN layer $l$ performs the following steps for each node $i$:
\begin{enumerate}
    \item \textbf{Message Computation:} Each neighbor $j$ of node $i$ computes a message, often based on its own features $h_j^{(l-1)}$ and potentially the edge features $e_{ji}$.
    \item \textbf{Aggregation:} Node $i$ aggregates the incoming messages from its neighbors (defined by the graph structure, often using a permutation-invariant function like sum, mean, or max).
    \item \textbf{Update:} Node $i$ updates its hidden representation $h_i^{(l)}$ based on its previous representation $h_i^{(l-1)}$ and the aggregated message.
\end{enumerate}
Mathematically, many GNN layers can be expressed compactly using GSOs. Let $H^{(l)} \in \mathbb{R}^{N \times F^{(l)}}$ be the matrix of node features at layer $l$ (with $H^{(0)} = X$, the input features). A generic GNN layer can often be written as:
\begin{equation}
    H^{(l+1)} = \sigma \left( \text{AGGREGATE} \left( H^{(l)}, G_t \right) \cdot W^{(l)} \right)
\end{equation}
where $\sigma$ is a non-linear activation function (e.g., ReLU), $W^{(l)}$ is a learnable weight matrix for feature transformation, and $\text{AGGREGATE}(H^{(l)}, G_t)$ represents the neighborhood aggregation operation dictated by the graph $G_t$ (often involving a GSO).

\subsection{Graph Convolutional Network (GCN)}
The Graph Convolutional Network (GCN) \cite{Kipf2017GCN} is a popular and foundational GNN architecture. Its layer-wise propagation rule is:
\begin{equation}
    H^{(l+1)} = \sigma \left( \tilde{A}_t H^{(l)} W^{(l)} \right)
    \label{eq:gcn_layer}
\end{equation}
Here, the aggregation step is performed by multiplying with the symmetrically normalized adjacency matrix with self-loops, $\tilde{A}_t$. This effectively computes a weighted average of the features of a node and its immediate neighbors (1-hop neighborhood).

\subsection{Fixed Neighborhood Limitation}
A key characteristic of standard GCN layers (Eq. \ref{eq:gcn_layer}) is that they only aggregate information from the immediate 1-hop neighborhood in a single step. To capture information from nodes further away (K-hops), one typically needs to stack $K$ GCN layers. Alternatively, some architectures use graph polynomial filters \cite{Defferrard2016ChebNet}, where a single layer aggregates information over K-hops using powers of the GSO 
\begin{equation}
    H_{GNN} = f_{GNN}(X_t, S_t; K, \theta_{GNN}) = \sigma \left( \sum_{k=0}^{K-1} c_k S_t^k X_t W_k \right) \quad \text{(Conceptual form)}
\end{equation}
where $S_t$ is a GSO, $K$ defines the maximum hop count (filter degree), and $c_k, W_k$ are learnable parameters.

In both stacking layers and using polynomial filters, the extent of the neighborhood ($K$) is typically \textbf{fixed} and predefined. As discussed previously, this fixed nature is a major limitation in dynamic MRPP scenarios where the optimal communication range might vary.

\section{Graph Signal Processing and Diffusion Processes}
\label{sec:gsp_diffusion}

Graph Signal Processing (GSP) extends classical signal processing concepts to data defined on graph structures. It provides tools to analyze graph signals (features on nodes) in terms of their variation and smoothness with respect to the underlying graph topology.

\subsection{Graph Fourier Transform}
The eigendecomposition of a graph Laplacian matrix (e.g., $L_{norm} = U \Lambda U^T$, where $U$ contains the eigenvectors and $\Lambda$ is a diagonal matrix of eigenvalues) defines the Graph Fourier Transform (GFT). The eigenvectors $U$ form an orthonormal basis representing different modes of variation (graph frequencies), and the eigenvalues $\Lambda$ represent the corresponding frequencies. Small eigenvalues correspond to low frequencies (smooth signals over the graph), while large eigenvalues correspond to high frequencies (signals with rapid variations between neighbors).

\subsection{Diffusion Processes on Graphs}
Diffusion processes model the spread or flow of information (or heat, particles, etc.) over a network. A fundamental diffusion process on a graph, analogous to the heat equation in continuous space, can be described by the differential equation:
\begin{equation}
    \frac{dX(t)}{dt} = -L X(t)
    \label{eq:diffusion_pde}
\end{equation}
where $X(t) \in \mathbb{R}^{N \times F}$ represents the features/signal on the graph nodes at diffusion time $t$, and $L$ is a graph Laplacian operator.

\subsection{Graph Heat Kernel}
The solution to the diffusion equation (Eq. \ref{eq:diffusion_pde}) with an initial condition $X(0)$ is given by:
\begin{equation}
    X(t) = e^{-tL} X(0)
    \label{eq:diffusion_solution}
\end{equation}
The matrix exponential $H_t = e^{-tL}$ is known as the \textbf{graph heat kernel}. It acts as a linear operator that propagates the initial signal $X(0)$ over the graph structure for a diffusion time $t$.

Key properties of the heat kernel $H_t$:
\begin{itemize}
    \item \textbf{Low-pass Filter:} From a spectral perspective (using GFT), the heat kernel applies a filter $h(\lambda) = e^{-t\lambda}$ to the graph frequencies $\lambda$ (eigenvalues of $L$). Since $L$ is positive semi-definite ($\lambda \ge 0$), this filter attenuates high frequencies more strongly than low frequencies, effectively smoothing the signal over the graph.
    \item \textbf{Controlling Diffusion Extent:} The diffusion time parameter $t \ge 0$ controls the extent of smoothing and information propagation:
        \begin{itemize}
            \item As $t \to 0$, $H_t \to I$. There is no diffusion; each node retains only its initial information.
            \item As $t \to \infty$, $H_t$ projects the signal onto the graph's connected components' average values (assuming $L$ corresponds to a connected graph's Laplacian). Information spreads globally within components.
            \item For intermediate $t$, $H_t$ aggregates information from a neighborhood whose effective "radius" increases with $t$. This provides a principled way to control the scale of feature aggregation.
        \end{itemize}
\end{itemize}

\section{Adaptive Diffusion Convolution (ADC)}
\label{sec:adc}

The limitations of fixed-K GNNs motivate the use of more flexible aggregation mechanisms. Graph Diffusion Convolution (GDC) \cite{Klicpera2019DiffusionGCN} proposed using generalized diffusion processes (including PageRank and heat kernel) for feature propagation, but typically requires manual tuning of the diffusion parameters (like $t$ for the heat kernel) for each dataset.

Adaptive Diffusion Convolution (ADC) \cite{Zhao2021ADC} builds upon this by making the diffusion parameters \textbf{learnable}, allowing the network to automatically adapt the neighborhood size during training.

\subsection{ADC with Heat Kernel}
Focusing on the heat kernel, ADC employs $H_t = e^{-tL}$ as the feature propagation mechanism, where $L$ is typically chosen as $L = I - T$ with $T$ being a suitable GSO (e.g., $T=\tilde{A}_t$). The crucial difference is that the diffusion time $t$ is treated as a learnable parameter, optimized via backpropagation along with the network weights $W^{(l)}$.

\subsection{Implementation via Taylor Approximation}
Directly computing the matrix exponential $e^{-tL}$ is computationally expensive for large graphs. ADC relies on the Taylor series expansion. Since $L = I - T$, the heat kernel is $H_t = e^{-t(I-T)} = e^{-t} e^{tT}$. The term $e^{tT}$ can be approximated by truncating its Taylor series expansion:
\begin{equation}
    e^{tT} = \sum_{k=0}^{\infty} \frac{(tT)^k}{k!} \approx \sum_{k=0}^{K_{trunc}} \frac{t^k}{k!} T^k
\end{equation}
Combining these, the practical ADC propagation operator based on the truncated heat kernel becomes (similar to Eq. 10 in the concise paper):
\begin{equation}
    H_t \approx \sum_{k=0}^{K_{trunc}} \theta_k(t) T^k \quad \text{where} \quad \theta_k(t) = e^{-t} \frac{t^k}{k!}
    \label{eq:adc_approx_original} % Renamed to avoid conflict if needed elsewhere
\end{equation}
Here, $K_{trunc}$ is the truncation order (maximum hop distance considered in the approximation), and $T$ is the chosen GSO (e.g., $T = \tilde{A}_t$). The key is that the coefficients $\theta_k(t)$ depend on the \textit{learnable} parameter $t$. This allows the GNN layer using ADC:
\begin{equation}
    H^{(l+1)} = \sigma \left( \left( \sum_{k=0}^{K_{trunc}} \theta_k(t) T^k \right) H^{(l)} W^{(l)} \right)
\end{equation}
to adaptively control the influence of neighbors at different hop distances $k$ by learning the optimal diffusion time $t$.


% --- START OF REPLACEMENT ---
% \subsection{Theoretical Advantages}
% As highlighted in the concise paper (Sec VI) and \cite{Zhao2021ADC}:
% \begin{itemize}
%     \item \textbf{Stability:} If the chosen GSO $T$ satisfies $\|T\| \le 1$ (which $\tilde{A}_t$ does), the operator norm of the ADC propagation matrix (Eq. \ref{eq:adc_approx_original}) is bounded: $\|\sum_{k=0}^{K_{trunc}} \theta_k(t) T^k \| \le \sum_{k=0}^{K_{trunc}} |\theta_k(t)| \|T\|^k \le \sum_{k=0}^{\infty} \theta_k(t) = e^{-t}e^t = 1$. This inherent stability contrasts with standard GCN layers where the weights $W^{(l)}$ could potentially lead to exploding activations if not properly regularized.
%     \item \textbf{Adaptive Filtering:} Learning $t$ effectively means learning the cutoff frequency or bandwidth of the underlying low-pass heat kernel filter ($h(\lambda; t)=e^{-t\lambda}$). This allows the model to adapt the degree of signal smoothing based on the data and task, rather than using a fixed filter shape defined by K.
% \end{itemize}

% This ADC mechanism, particularly the learnable diffusion time $t$, forms the core of the adaptive communication module integrated into the MRPP framework proposed in this thesis (Chapter \ref{chap:methodology}). It replaces the fixed K-hop aggregation of standard GNNs, enabling more flexible and potentially more effective information sharing among robots.

% --- END OF REPLACEMENT ---

% --- START OF NEW SECTION ---
\section{Principal Technical Results}
\label{sec:principal_results}

This section provides a more detailed comparison of the theoretical properties, specifically stability and spectral filtering characteristics, between fixed-\(K\) Graph Neural Networks (like GCN) and the Adaptive Diffusion Convolution (ADC) approach utilized in this work.And these were the mathematical proof which shows why the ADC approach was chosen over the GCN approach.

\subsection{Stability Analysis}
We analyze the stability of the linear propagation operators inherent in GCN and ADC layers. Stability ensures that the magnitude of the node features does not grow uncontrollably through layers, which is crucial for reliable training and performance.
\newpage
\subsubsection{Notation and Assumptions for Analysis}
For the following analysis, let:
\begin{itemize}
    \item $\mathbf{S} \in \mathbb{R}^{N \times N}$ be a normalized graph shift operator (GSO), such as the symmetrically normalized adjacency matrix $\tilde{A}_t$ defined earlier, with spectral radius $\rho(\mathbf{S}) \le 1$. The spectral radius is the maximum absolute eigenvalue.
    \item $\mathbf{L} = \mathbf{I} - \mathbf{S}$ be the corresponding normalized Laplacian matrix. Its eigenvalues $\lambda_i$ satisfy $0 \le \lambda_i \le 2$. We can write its eigendecomposition as $\mathbf{L} = U \Lambda U^\top$, where $\Lambda = \mathrm{diag}(\lambda_1, \dots, \lambda_N)$.
    \item $\mathbf{T}$ be the propagation matrix used in ADC, assumed to satisfy $\|\mathbf{T}\| \le 1$. A common choice is $\mathbf{T} = \mathbf{S}$.
    \item $\|\cdot\|$ denote the induced $\ell_2$ operator norm (spectral norm). For symmetric matrices like $\mathbf{S}$ and $\mathbf{L}$, this is equal to the largest absolute eigenvalue.
\end{itemize}

\subsubsection{GCN Stability}
The linear part of a multi-hop GCN layer or a stack of GCN layers can often be represented as a polynomial filter applied to the node features:
\begin{equation}
P_{\mathrm{GCN}} = \sum_{k=0}^{K-1} \alpha_k \mathbf{S}^k
\label{eq:pgcn_poly}
\end{equation}
Here, $K$ represents the number of hops (or layers), $\mathbf{S}$ is the graph shift operator (GSO), and $\{\alpha_k\}$ are scalar coefficients effectively determined by the learned weight matrices $W^{(l)}$ across layers and any specific polynomial filter formulation (like ChebNet or the simplified GCN form). The stability of this operator is assessed by its operator norm, $\|P_{\mathrm{GCN}}\|$.

We can bound this norm as follows:
\begin{align}
    \|P_{\mathrm{GCN}}\| &= \left\| \sum_{k=0}^{K-1} \alpha_k \mathbf{S}^k \right\| && \text{(Definition from Eq. \ref{eq:pgcn_poly})} \nonumber \\
    &\le \sum_{k=0}^{K-1} \left\| \alpha_k \mathbf{S}^k \right\| && \text{(Triangle inequality for operator norms)} \nonumber \\
    &= \sum_{k=0}^{K-1} |\alpha_k| \left\| \mathbf{S}^k \right\| && \text{(Property of operator norms: } \|\beta \mathbf{X}\| = |\beta| \|\mathbf{X}\| \text{ for scalar } \beta \text{)} \label{eq:gcn_bound_step1}
\end{align}
Now, we consider the term $\|\mathbf{S}^k\|$. We use the submultiplicative property of operator norms, which states that $\|\mathbf{A}\mathbf{B}\| \le \|\mathbf{A}\|\|\mathbf{B}\|$. Applying this repeatedly:
\begin{equation}
    \|\mathbf{S}^k\| = \|\underbrace{\mathbf{S} \cdot \mathbf{S} \cdots \mathbf{S}}_{k \text{ times}}\| \le \underbrace{\|\mathbf{S}\| \cdot \|\mathbf{S}\| \cdots \|\mathbf{S}\|}_{k \text{ times}} = \|\mathbf{S}\|^k
    \label{eq:norm_sk_bound}
\end{equation}
In GCNs and many GNNs, the graph shift operator $\mathbf{S}$ (e.g., the symmetrically normalized adjacency matrix $\tilde{A}_t$) is typically normalized such that its spectral radius $\rho(\mathbf{S}) \le 1$. For symmetric matrices (like $\tilde{A}_t$), the operator norm (specifically, the 2-norm) is equal to its spectral radius, i.e., $\|\mathbf{S}\|_2 = \rho(\mathbf{S})$. Thus, if $\rho(\mathbf{S}) \le 1$, then $\|\mathbf{S}\| \le 1$. If $\mathbf{S}$ is not symmetric but is a normal matrix, the same holds. Even for general matrices, if we are considering the 2-norm, it's bounded by singular values, but the common normalization aims for $\rho(\mathbf{S}) \le 1$.

Assuming $\|\mathbf{S}\| \le 1$, then from Eq. \ref{eq:norm_sk_bound}:
\begin{equation}
    \|\mathbf{S}\|^k \le 1^k = 1
    \label{eq:norm_s_power_one}
\end{equation}
Therefore, $\|\mathbf{S}^k\| \le 1$.

Substituting this back into Eq. \ref{eq:gcn_bound_step1}:
\begin{equation}
    \|P_{\mathrm{GCN}}\| \le \sum_{k=0}^{K-1} |\alpha_k| (1) = \sum_{k=0}^{K-1} |\alpha_k|
    \label{eq:pgcn_final_bound}
\end{equation}
The last inequality (from the original text) holds because $\|\mathbf{S}^k\| \le \|\mathbf{S}\|^k \le 1^k = 1$ since $\mathbf{S}$ is typically normalized such that its spectral radius $\rho(\mathbf{S}) \le 1$ (implying $\|\mathbf{S}\| \le 1$ if $\mathbf{S}$ is symmetric or normal).

However, the crucial point is that the coefficients $\{\alpha_k\}$ are learned and generally \textit{unconstrained}. During training, the sum $\sum_{k=0}^{K-1} |\alpha_k|$ can potentially grow very large, leading to $\|P_{\mathrm{GCN}}\|$ exceeding 1 (as per the bound in Eq. \ref{eq:pgcn_final_bound}). This can cause exploding gradients or activations, making the network unstable without careful regularization or initialization strategies.
\subsubsection{ADC Stability}
ADC employs a propagation operator based on the truncated heat-kernel expansion, with coefficients determined by a \textit{learnable} diffusion time $t \ge 0$:
\begin{equation}
P_{\mathrm{ADC}}(t) = \sum_{k=0}^{K_{\mathrm{trunc}}} \theta_k(t)\,\mathbf{T}^k, \quad \text{where} \quad \theta_k(t) = e^{-t}\,\frac{t^k}{k!}
\end{equation}
Here, $K_{\mathrm{trunc}}$ is the truncation order of the Taylor series approximation for the heat kernel.

\begin{theorem}\label{thm:adc_stability}[ADC Stability Bound]
If the propagation matrix $\mathbf{T}$ satisfies $\|\mathbf{T}\| \le 1$, then for any diffusion time $t \ge 0$ and any truncation order $K_{\mathrm{trunc}} \ge 0$, the operator norm of the ADC propagation matrix is bounded by 1:
\begin{equation}
\|P_{\mathrm{ADC}}(t)\| \;\le\; 1.
\end{equation}
\end{theorem}

\begin{proof}
Using the triangle inequality and properties of the operator norm:
\begin{equation}
\|P_{\mathrm{ADC}}(t)\| = \left\|\sum_{k=0}^{K_{\mathrm{trunc}}}\theta_k(t)\,\mathbf{T}^k\right\| \;\le\; \sum_{k=0}^{K_{\mathrm{trunc}}}\left|\theta_k(t)\right|\,\|\mathbf{T}^k\|
\end{equation}
Since $t \ge 0$, the coefficients $\theta_k(t) = e^{-t} \frac{t^k}{k!}$ are non-negative, so $|\theta_k(t)| = \theta_k(t)$. Also, since $\|\mathbf{T}\| \le 1$, we have $\|\mathbf{T}^k\| \le \|\mathbf{T}\|^k \le 1^k = 1$. Therefore:
\begin{equation}
\|P_{\mathrm{ADC}}(t)\| \;\le\; \sum_{k=0}^{K_{\mathrm{trunc}}}\theta_k(t) \cdot 1 = \sum_{k=0}^{K_{\mathrm{trunc}}}\theta_k(t)
\end{equation}
This sum is a partial sum of the Taylor series for $e^t$, multiplied by $e^{-t}$. Since all terms are non-negative, the partial sum is less than or equal to the infinite sum:
\begin{equation}
\sum_{k=0}^{K_{\mathrm{trunc}}}\theta_k(t) \;\le\; \sum_{k=0}^{\infty}\theta_k(t) = \sum_{k=0}^{\infty} e^{-t}\,\frac{t^k}{k!} = e^{-t} \sum_{k=0}^{\infty} \frac{t^k}{k!} = e^{-t} e^{t} = 1
\end{equation}
Combining these steps, we get $\|P_{\mathrm{ADC}}(t)\| \le 1$.
\end{proof}

\noindent This result shows that the ADC aggregation mechanism is \textbf{inherently stable} due to the structure of the heat kernel coefficients. The operator norm is guaranteed to be bounded by 1, preventing the explosion of feature magnitudes regardless of the learned value of $t$ or the chosen truncation $K_{\rm trunc}$. This contrasts sharply with the potentially unbounded norm of the GCN operator.

\subsection{Spectral Filtering Characteristics}
We now analyze how GCN and ADC layers process graph signals in the frequency domain, defined by the eigenvalues $\lambda$ of the graph Laplacian $\mathbf{L}$. The behavior of a layer is characterized by its frequency response $h(\lambda)$.

\subsubsection{GCN as Polynomial Filter}
Consider the common GCN formulation using the normalized adjacency matrix $\mathbf{S} = \mathbf{I} - \mathbf{L}$. The propagation operator $P_{\mathrm{GCN}} = \sum_{k=0}^{K-1} c_k \mathbf{S}^k$ acts on the graph signal spectrum. Using the eigendecomposition $\mathbf{L} = U \Lambda U^\top$ (and thus $\mathbf{S} = U (\mathbf{I} - \Lambda) U^\top$), the spectral response of the GCN operator corresponding to an eigenvalue $\lambda$ of $\mathbf{L}$ is:
\begin{equation}
h_{\mathrm{GCN}}(\lambda) = \sum_{k=0}^{K-1} c_k (1 - \lambda)^k
\end{equation}
This shows that the GCN layer implements a graph filter whose frequency response is a \textbf{polynomial} in $(1-\lambda)$ of degree $K-1$. While the learnable coefficients $\{c_k\}$ offer flexibility in shaping the filter, the filter's complexity (degree) is \textbf{fixed} by the chosen number of hops/layers $K$. Selecting an appropriate $K$ is often crucial and problem-dependent.

\subsubsection{ADC as Exponential Filter}
The ideal (non-truncated) graph heat kernel is defined as the matrix exponential $\exp(-t\mathbf{L})$. Its spectral response can be found using the functional calculus on the Laplacian eigenvalues:
\begin{equation}
h_{\mathrm{ADC, ideal}}(\lambda; t) = e^{-t\lambda}
\end{equation}
The ADC layer, using the truncated approximation $P_{\mathrm{ADC}}(t) \approx \exp(-t(\mathbf{I}-\mathbf{T}))$, aims to approximate this behavior. The function $e^{-t\lambda}$ is a classic \textbf{low-pass filter}. For $\lambda=0$ (corresponding to the constant signal on connected components), the response is $e^0 = 1$. As the frequency $\lambda$ increases, the response decays exponentially.

\begin{theorem}\label{thm:adc_adaptivity}[ADC Filter Adaptivity]
The spectral response $h_{\mathrm{ADC, ideal}}(\lambda;t) = e^{-t\lambda}$ corresponds to a low-pass filter whose cutoff frequency and steepness are continuously controlled by the diffusion time parameter $t$. By learning $t$ through backpropagation, the ADC layer can adapt the filter's bandwidth (i.e., the degree of smoothing or the effective neighborhood size) to best suit the data and task.
\end{theorem}

\begin{proof}
Using the eigendecomposition $\mathbf{L}=U\Lambda U^\top$, the matrix exponential is $\exp(-t\mathbf{L}) = U\,\exp(-t\Lambda)\,U^\top$. The matrix $\exp(-t\Lambda)$ is diagonal with entries $e^{-t\lambda_i}$.
For a fixed $t > 0$:
\begin{itemize}
    \item If $\lambda_i$ is small (low frequency), $e^{-t\lambda_i}$ is close to 1, preserving the signal component.
    \item If $\lambda_i$ is large (high frequency), $e^{-t\lambda_i}$ is close to 0, attenuating the signal component.
\end{itemize}
The transition between passing and attenuating frequencies depends on $t$. As $t$ increases, the exponential decay becomes steeper, meaning the filter becomes more aggressively low-pass, suppressing higher frequencies more strongly and effectively averaging over larger neighborhoods. Conversely, as $t \to 0$, $e^{-t\lambda} \to 1$ for all $\lambda$, approaching an identity filter (no aggregation). Since $t$ is learned, the model can choose the optimal level of smoothing/aggregation.
\end{proof}

The truncated ADC operator $P_{\mathrm{ADC}}(t)$ provides a polynomial approximation to this ideal exponential filter, still retaining the adaptive control via the learnable $t$.

\subsection{Summary of Theoretical Advantages of ADC over Fixed-K GCN}
Based on the analysis above, ADC offers significant theoretical advantages:

\begin{enumerate}
    \item \textbf{Guaranteed Stability:} ADC's propagation operator norm is inherently bounded by 1 (Theorem \ref{thm:adc_stability}), preventing issues related to exploding feature norms without requiring explicit regularization on the propagation weights. GCN operators lack this guarantee as their norm depends on potentially unbounded learned coefficients. This inherent stability can lead to more robust training.
    \item \textbf{Adaptive Spectral Filtering:} ADC implements a graph filter based on the heat kernel $e^{-t\lambda}$, whose low-pass characteristics (bandwidth, neighborhood size) are continuously controlled by the \textit{learnable} parameter $t$ (Theorem \ref{thm:adc_adaptivity}). This allows the model to automatically adapt the extent of information aggregation based on the task and data distribution. GCN, in contrast, uses a polynomial filter whose degree (complexity) is fixed by the predetermined hyperparameter $K$, limiting its adaptability.
\end{enumerate}

These properties suggest that ADC can provide a more robust, stable, and adaptive mechanism for information propagation in GNNs compared to standard fixed-K GCN layers, potentially leading to better performance, especially in dynamic scenarios like MRPP where the optimal communication range might vary.


% --- END OF NEW SECTION ---


% End of Chapter 3
% Add bibliography command here in your main file: \bibliography{your_bib_file}